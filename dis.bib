@techreport{verizon,
title = {{Verizon's 2016 Data Breach Investigations Report}},
url = {http://www.verizonenterprise.com/verizon-insights-lab/dbir/2016/},
year = "2017"
}
@unpublished{cybox,
title = {{Cyber Observable eXpression}},
url = {https://cyboxproject.github.io/},
urldate = {2017},
year = "2017"
}
@unpublished{taxii,
title = {{Trusted Automated eXchange of Indicator Information}},
url = {https://taxiiproject.github.io/},
year = "2017"
}
@unpublished{snort,
title = {{Snort rules}},
url = {https://www.snort.org/},
urldate = {2017},
year = "2017"

}
@unpublished{yara,
title = {{Yara rules}},
url = {https://virustotal.github.io/yara/},
urldate = {2017},
year = "2017"
}
@unpublished{symantecblog,
title = {{Symantec blog}},
url = {https://www.symantec.com/connect/blogs},
urldate = {2017},
year = "2017"
}
@unpublished{cozyduke,
title = {{CozyDuke Apt Report}},
url = {https://www.f-secure.com/documents/996508/1030745/CozyDuke},
year = "2017"
}
@unpublished{kaspersky,
title = {{Kaspersky White Papers}},
url = {http://usa.kaspersky.com/enterprise-security/resources/white-papers},
year = "2017"
}
@unpublished{apt,
title = {{APTNotes database}},
url = {https://github.com/aptnotes/data},
urldate = {2017},
year = "2017"
}
@unpublished{beautifulsoup,
title = {{Beautiful Soup}},
url = {https://www.crummy.com/software/BeautifulSoup/},
urldate = {2017},
year = "2017"
}
@unpublished{tensorflowapi,
title = {{Tensorflow Api docs}},
url = {https://www.tensorflow.org/api{\_}guides/python/nn},
year = "2017"
}
@unpublished{misp,
title = {{Malware Information Sharing Platform (MISP)}},
url = {http://www.misp-project.org/},
urldate = {2017},
year = "2017"
}
@unpublished{stix,
title = {{Structured Threat Information eXpression}},
url = {https://stixproject.github.io/},
urldate = {2017},
year = "2017"
}
@unpublished{suricata,
title = {{Suricata rules}},
url = {https://suricata-ids.org/},
urldate = {2017},
year = "2017"
}
@unpublished{openioc,
title = {{The OpenIOC Framework.}},
url = {http://www.openioc.org/},
urldate = {2017},
year = "2017"
}
@book{maec,
title = {{Malware Attribute Enumeration Characterization}},
url = {https://maec.mitre.org/},
urldate = {2017},
year = "2017"
}
@unpublished{capec,
title = {{Common Attack Pattern Enumeration and Classification}},
url = {https://capec.mitre.org/},
urldate = {2017},
year = "2017"
}
@unpublished{otx,
title = {{Alien Vault Open Threat Exchange}},
url = {https://otx.alienvault.com/},
urldate = {2017},
year = "2017"
}
@unpublished{pdfminer,
title = {{PDFMiner}},
url = {http://www.unixuser.org/{~}euske/python/pdfminer/},
urldate = {2017},
year = "2017"
}
@unpublished{openstack,
title = {{Openstack}},
url = {https://www.openstack.org/},
urldate = {2017},
year = "2017"
}
@article{zou,
abstract = {We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n).By contrast, the lasso is not a very satisfactory variable selection method in the p{\textgreater}{\textgreater}n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
isbn = {1369-7412},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Grouping effect,LARS algorithm,Lasso,P ≫ n problem,Penalization,Variable selection},
number = {2},
pages = {301--320},
pmid = {20713001},
title = {{Regularization and variable selection via the elastic net}},
volume = {67},
year = {2005}
}
@article{yih,
abstract = {Traditional text similarity measures consider each term similar only to itself and do not model semantic relatedness of terms. We pro- pose a novel discriminative training method that projects the raw term vectors into a com- mon, low-dimensional vector space. Our ap- proach operates by finding the optimal matrix to minimize the loss of the pre-selected sim- ilarity function (e.g., cosine) of the projected vectors, and is able to efficiently handle a large number of training examples in the high- dimensional space. Evaluated on two very dif- ferent tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art ap- proaches, but also achieves high accuracy at low dimensions and is thus more efficient.},
author = {Yih, W and Toutanova, K and Platt, Jc and Meek, C},
isbn = {9781932432923},
issn = {9781932432923},
journal = {Proceedings of the Fifteenth Conference on Computational Natural Language Learning},
pages = {247--256},
title = {{Learning discriminative projections for text similarity measures}},
url = {http://dl.acm.org/citation.cfm?id=2018965},
year = {2011}
}
@inproceedings{stone,
abstract = {Most personal photos that are shared online are embedded in some form of social network, and these social networks are a potent source of contextual information that can be leveraged for automatic image understanding. In this paper, we investigate the utility of social network context for the task of automatic face recognition in personal photographs. We combine face recognition scores with social context in a conditional random field (CRF) model and apply this model to label faces in photos from the popular online social network Facebook, which is now the top photo-sharing site on the Web with billions of photos in total. We demonstrate that our simple method of enhancing face recognition with social network context substantially increases recognition performance beyond that of a baseline face recognition system.},
author = {Stone, Zak and Zickler, Todd and Darrell, Trevor},
booktitle = {2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops},
doi = {10.1109/CVPRW.2008.4562956},
isbn = {9781424423408},
issn = {2160-7508},
title = {{Autotagging Facebook: Social network context improves photo annotation}},
year = {2008}
}
@article{srivastava,
author = {Srivastava, N and Hinton, G and Krizhevsky, A and Sutskever, I and Salakhutdinov, R},
journal = {JMLR},
pages = {pp. 1929--1958},
title = {{Dropout: A simple way to prevent neural networks from overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{sekar,
author = {Sekar, R and Uppuluri, P},
booktitle = {Proceedings of the USENIX Security Symposium},
title = {{Synthesizing Fast Intrusion Prevention/Detection Systems from High-Level Specifications}},
year = {1999}
}
@article{rumelhart,
author = {Rumelhart, D and Hinton, G and Williams, R},
journal = {Nature},
pages = {pp. 533--536},
title = {{Learning Representations by Back-Propagating Errors}},
volume = {323},
year = {1986}
}
@inproceedings{ranzato,
abstract = {We present an unsupervised method for learning a hi- erarchy of sparse feature detectors that are invariant to small shifts and distortions. The resulting feature extrac- tor consists of multiple convolution filters, followed by a feature-pooling layer that computes the max of each fil- ter output within adjacent windows, and a point-wise sig- moid non-linearity. A second level of larger and more in- variant features is obtained by training the same algorithm on patches of features from the first level. Training a su- pervised classifier on these features yields 0.64{\%} error on MNIST, and 54{\%} average recognition rate on Caltech 101 with 30 training samples per category. While the result- ing architecture is similar to convolutional networks, the layer-wise unsupervised training procedure alleviates the over-parameterization problems that plague purely super- vised learning procedures, and yields good performance with very few labeled training samples.},
author = {Ranzato, Marc'Aurelio and Huang, Fu Jie and Boureau, Y. Lan and LeCun, Yann},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2007.383157},
isbn = {1424411807},
issn = {10636919},
pmid = {4270182},
title = {{Unsupervised learning of invariant feature hierarchies with applications to object recognition}},
year = {2007}
}
@inproceedings{pennington,
author = {Pennington, J and Socher, R and Manning, C D},
booktitle = {Proceedings of the Empiricial Methods in Natural Language Processing},
title = {{Glove: Global vectors for word representation}},
year = {2014}
}
@book{oktavianto,
author = {Oktavianto, D and Muhardianto, I},
publisher = {Packt Pbl. Ltd},
title = {{Cuckoo Malware Analysis}},
year = {2013}
}
@book{gorman,
author = {O'Gorman, G and McDonald, G},
publisher = {Symantec Corporation},
title = {{Ransomware: A growing menace (Technical report)}},
year = {2012}
}
@inproceedings{obrst,
abstract = {This paper reports on a trade study we performed to support the development of a Cyber ontology from an initial malware ontology. The goals of the Cyber ontology effort are first described, followed by a discussion of the ontology development methodology used. The main body of the paper then follows, which is a description of the potential ontologies and standards that could be utilized to extend the Cyber ontology from its initially constrained malware focus. These resources include, in particular, Cyber and malware standards, schemas, and terminologies that directly contributed to the initial malware ontology effort. Other resources are upper (sometimes called 'foundational') ontologies. Core concepts that any Cyber ontology will extend have already been identified and rigorously defined in these foundational ontologies. However, for lack of space, this section is profoundly reduced. In addition, utility ontologies that are focused on time, geospatial, person, events, and network operations are briefly described. These utility ontologies can be viewed as specialized super-domain or even mid-level ontologies, since they span many, if not most, ontologies -- including any Cyber ontology. An overall view of the ontological architecture used by the trade study is also given. The report on the trade study concludes with some proposed next steps in the iterative evolution of the Cyber},
author = {Obrst, Leo and Chase, Penny and Markeloff, Richard},
booktitle = {CEUR Workshop Proceedings},
isbn = {9781880843864},
issn = {16130073},
keywords = {Cyber,Malware,Ontology,Trade study},
pages = {49--56},
title = {{Developing an ontology of the cyber security domain}},
volume = {966},
year = {2014}
}
@inproceedings{ng,
author = {Ng, A},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
title = {{Feature selection, {\$}l{\_}1{\$} vs. {\$}l{\_}2{\$} regularization, and rotational invariance.}},
year = {2004}
}
@article{nair,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
eprint = {1111.6189v1},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@inproceedings{mikolov,
author = {Mikolov, T and Sutskever, I and Chen, K and Corrado, G and Dean, J},
booktitle = {Proceedings of Neural Information Processing Systems},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@article{rob,
author = {McMillan, R},
title = {{Open Threat Intelligence}},
url = {https://www.gartner.com/doc/2487216/definition-threat-intelligence},
volume = {2013}
}
@article{lunt,
author = {Lunt, T F},
journal = {In: Computers {\&} Security},
number = {4},
pages = {pp. 405--418},
title = {{A survey of intrusion detection techniques}},
volume = {12},
year = {1993}
}
@article{liao1,
author = {Liao, Y and Vemuri, V},
journal = {Computers and Security.},
number = {5},
pages = {pp. 439--448},
title = {{Use of K-Nearest Neighbor classifier for intrusion detection.}},
volume = {21},
year = {2002}
}
@inproceedings{liao,
abstract = {To adapt to the rapidly evolving landscape of cyber threats, secu-rity professionals are actively exchanging Indicators of Compro-mise (IOC) (e.g., malware signatures, botnet IPs) through public sources (e.g. blogs, forums, tweets, etc.). Such information, of-ten presented in articles, posts, white papers etc., can be converted into a machine-readable OpenIOC format for automatic analysis and quick deployment to various security mechanisms like an in-trusion detection system. With hundreds of thousands of sources in the wild, the IOC data are produced at a high volume and veloc-ity today, which becomes increasingly hard to manage by humans. Efforts to automatically gather such information from unstructured text, however, is impeded by the limitations of today's Natural Lan-guage Processing (NLP) techniques, which cannot meet the high standard (in terms of accuracy and coverage) expected from the IOCs that could serve as direct input to a defense system. In this paper, we present iACE, an innovation solution for fully automated IOC extraction. Our approach is based on the obser-vation that the IOCs in technical articles are often described in a predictable way: being connected to a set of context terms (e.g., " download ") through stable grammatical relations. Leveraging this observation, iACE is designed to automatically locate a putative IOC token (e.g., a zip file) and its context (e.g., " malware " , " down-load ") within the sentences in a technical article, and further an-alyze their relations through a novel application of graph mining techniques. Once the grammatical connection between the tokens is found to be in line with the way that the IOC is commonly pre-sented, these tokens are extracted to generate an OpenIOC item that describes not only the indicator (e.g., a malicious zip file) but also its context (e.g., download from an external source). Running on 71,000 articles collected from 45 leading technical blogs, this new approach demonstrates a remarkable performance: it gener-ated 900K OpenIOC items with a precision of 95{\%} and a coverage over 90{\%}, which is way beyond what the state-of-the-art NLP tech-nique and industry IOC tool can achieve, at a speed of thousands of articles per hour. Further, by correlating the IOCs mined from the articles published over a 13-year span, our study sheds new light on 1 The two lead authors are ordered alphabetically.},
author = {Liao, Xiaojing and Yuan, Kan and Wang, Xiaofeng and Li, Zhou and Xing, Luyi and Beyah, Raheem},
booktitle = {CCS '16 Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
doi = {10.1145/2976749.2978315},
isbn = {9781450341394},
issn = {15437221},
pages = {755--766},
title = {{Acing the IOC Game : Toward Automatic Discovery and Analysis of Open-Source Cyber Threat Intelligence}},
year = {2016}
}
@article{krizhevsky,
author = {Krizhevsky, A and Sutskever, I and Hinton, G.:},
journal = {NIPS},
title = {{ImageNet classification with deep convolutional neural networks}},
year = {2012}
}
@inproceedings{kim,
author = {Kim, Y},
booktitle = {Proceedings of the Empiricial Methods in Natural Language Processing},
title = {{Convolutional neural networks for sentence classification}},
year = {2014}
}
@article{hu,
author = {Hu, B and Lu, Z and Li, H and Chen, Q.:},
journal = {NIPS},
pages = {pp. 2042--2050},
title = {{Convolutional neural network architectures for matching natural language sentences}},
year = {2014}
}
@article{hinton,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
doi = {arXiv:1207.0580},
eprint = {1207.0580},
isbn = {9781467394673},
issn = {9781467394673},
journal = {ArXiv e-prints},
pages = {1--18},
pmid = {1000104337},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@inproceedings{goodfellow,
abstract = {We consider the problem of designing mod-els to leverage a recently introduced ap-proximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a nat-ural companion to dropout) designed to both facilitate optimization by dropout and im-prove the accuracy of dropout's fast approxi-mate model averaging technique. We empir-ically verify that the model successfully ac-complishes both of these tasks. We use max-out and dropout to demonstrate state of the art classification performance on four bench-mark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
archivePrefix = {arXiv},
arxivId = {1302.4389},
author = {Goodfellow, Ian J and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
booktitle = {Proceedings of the 30th International Conference on Machine Learning (ICML)},
eprint = {1302.4389},
pages = {1319--1327},
title = {{Maxout Networks}},
volume = {28},
year = {2013}
}
@article{nir,
author = {Friedman, N and Geiger, D and Goldszmidt, M},
journal = {Machine Learning},
pages = {pp. 131--163},
title = {{Bayesian network classifiers.}},
year = {1997}
}
@article{friedman,
abstract = {Gradient boosting constructs additive regression models by sequentially fitting a simple parameterized function (base learner) to current "pseudo" -residuals by least squares at each iteration. The pseudo-residuals are the gradient of the loss functional being minimized, with respect to the model values at each training data point evaluated at the current step. It is shown that both the approximation accuracy and execution speed of gradient boosting can be substantially improved by incorporating randomization into the procedure. Specifically, at each iteration a subsample of the training data is drawn at random (without replacement) from the full training data set. This randomly selected subsample is then used in place of the full sample to fit the base learner and compute the model update for the current iteration. This randomized approach also increases robustness against overcapacity of the base learner. ?? 2002 Elsevier Science B.V. All rights reserved.},
author = {Friedman, Jerome H.},
doi = {10.1016/S0167-9473(01)00065-2},
isbn = {0167-9473},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
number = {4},
pages = {367--378},
title = {{Stochastic gradient boosting}},
volume = {38},
year = {2002}
}
@article{daly,
abstract = {Computer systems have become a very important part of our society, most of the information we use in our everyday lives is in its digital form, and since information is power it only makes sense that someone, somewhere will try to steal it. Attackers are adapting and now have access to highly sophisticated tools and expertise to conduct highly targeted and very complex attack campaigns. Advanced Persistent Threat, or APT, is a term coined by the United States Air Force around 2006 as a way to talk about classified intrusions with uncleared personnel. It wrongly and quickly became the standard acronym to describe every sort of attack. This work tries to demystify the problem of APTs, why they are called as such, and what are the most common tactics, techniques and procedures. It also discusses previously proposed life-cycles, profile the most common adversaries and takes a look at why traditional defences will not stop them. A big problem encountered while developing this work was the lack of statistics regarding APT attacks. One of the big contributions here consists on the search for publicly available reports, its analysis, and presentation of relevant information gathered in a summarised fashion. From the most targeted applications to the most typical infection vector, insight is given on how and why the adversaries conduct these attacks. Only after a clear understanding of the problem is reached, prevention and detection schemes were discussed. Specifically, blueprints for a system to be used by AnubisNetworks are presented, capable of detecting these attacks at the e-mail level. It is based on sandboxing and people mapping, which is a way to better understand people, one of the weakest links in security. The work is concluded by trying to understand how the threat landscape will shape itself in upcoming years.},
author = {Daly, Michael K},
doi = {10.1016/B978-1-59-749949-1.00009-7},
isbn = {9781597499491},
issn = {2056-4007},
journal = {Usenix, Nov},
keywords = {advanced persistent threats,intrusion detection systems,siem},
number = {4},
pages = {2013--2016},
title = {{Advanced persistent threat}},
volume = {4},
year = {2009}
}
@article{cortes,
author = {Cortes, C and Vapnik and V.},
journal = {Machine Learning},
pages = {pp. 273--297},
title = {{Support-vector networks.}},
year = {1995}
}
@article{collobert,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
doi = {10.1.1.231.4614},
eprint = {1103.0398},
isbn = {1532-4435},
issn = {0891-2017},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
pmid = {1000183096},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@article{claessen,
author = {Claessen, K and Duregard, J and Palka, M},
journal = {Functional and Logic Programming, Lecture Notes in Computer Science},
pages = {pp. 18--34},
title = {{Generating Constrained Random Data with Uniform Distribution}},
volume = {8475},
year = {2014}
}
@book{catakoglu,
author = {{Catakoglu O.}, Balduzzi M and Balzarotti, D},
publisher = {In WWW},
title = {{Automatic extraction of indicators of compromise for web applications}},
year = {2016}
}
@inproceedings{bojarski,
author = {Bojarski, M and Testa, D and Dworakowski, D and Firner, B and Flepp, B and Goyal, P and Jackel, L and Monfort, M and Zhang, M and Zhang, X and Zhao, J and K.:, Zieba},
booktitle = {arXiv preprint arXiv:1604.07316},
title = {{End to end learning for self-driving cars}},
year = {2016}
}
@article{bengio,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Janvin, Christian},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
isbn = {1532-4435},
issn = {15324435},
journal = {The Journal of Machine Learning Research},
keywords = {artificial neural networks,curse of dimensionality,distributed representation,statistical language modeling},
pages = {1137--1155},
pmid = {18244602},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@article{bayer,
abstract = {Anubis is a dynamic malware analysis platform that executes submitted binaries in a controlled environment. To perform the analysis, the system monitors the invocation of important Windows API calls and system services, it records the network traffic, and it tracks data flows. For each submission, reports are generated that provide comprehensive reports about the activities of the binary under analysis. Anubis receives malware samples through a public web interface and a number of feeds from security organizations and anti-malware companies. Because the samples are collected from a wide range of users, the collected samples represent a comprehensive and diverse mix of malware found in the wild. In this paper, we aim to shed light on common malware behaviors. To this end, we evaluate the Anubis analysis results for almost one million malware samples, study trends and evolution of malicious behaviors over a period of almost two years, and examine the influence of code polymorphism on malware statistics.},
author = {Bayer, Ulrich and Habibi, Imam and Balzarotti, Davide and Kirda, Engin and Kruegel, Christopher},
doi = {10.1109/BADGERS.2014.7},
isbn = {9781479983094},
journal = {Proceedings of the 2nd USENIX conference on Large-scale exploits and emergent threats: botnets, spyware, worms, and more},
pages = {8},
title = {{A view on current malware behaviors}},
url = {http://portal.acm.org/citation.cfm?id=1855676.1855684},
year = {2009}
}

@incollection{andress,
author = {Andress, J},
booktitle = {Journal Information Systems Security Association (ISSA) 5},
title = {{Working with Indicators of Compromise}},
year = {2015}
}
@article{abadi,
abstract = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. TensorFlow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with particularly strong support for training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model in contrast to existing systems, and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
archivePrefix = {arXiv},
arxivId = {1605.08695},
author = {Abadi, Mart{\'{i}}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1605.08695},
isbn = {978-1-931971-33-1},
journal = {Google Brain},
pages = {18},
title = {{TensorFlow: A system for large-scale machine learning}},
url = {http://arxiv.org/abs/1605.08695},
year = {2016}
}
